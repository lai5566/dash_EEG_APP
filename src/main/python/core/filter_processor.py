"""EEGä¿¡è™Ÿçš„å„ªåŒ–æ¿¾æ³¢è™•ç†å™¨"""

import numpy as np
import threading
import time
from typing import Dict, List, Tuple, Optional, Any
from concurrent.futures import ThreadPoolExecutor
from scipy.signal import butter, sosfiltfilt
import logging

logger = logging.getLogger(__name__)


class OptimizedFilterProcessor:
    """EEGä¿¡è™Ÿçš„å„ªåŒ–ä¸¦è¡Œæ¿¾æ³¢è™•ç†å™¨"""
    
    def __init__(self, sample_rate: int = 512, max_workers: int = 4):
        self.sample_rate = sample_rate
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
        self.cache = {}
        self.last_data_hash = None
        self.lock = threading.Lock()
        
        # EEGé »ç‡å¸¶
        self.bands = {
            "delta": (0.5, 4),
            "theta": (4, 8),
            "alpha": (8, 12),
            "beta": (12, 35),
            "gamma": (35, 50),
            "smr": (12, 15),  # æ„Ÿè¦ºé‹å‹•ç¯€å¾‹
            "mu": (8, 13),    # é‹å‹•çš®è³ªç¯€å¾‹
            "high_gamma": (50, 80)  # é«˜ä¼½é¦¬æ³¢ (ç‚ºæ€§èƒ½é™åˆ¶)
        }
        
        # é è¨ˆç®—æ¿¾æ³¢å™¨ä»¥æå‡æ€§èƒ½
        self.sos_filters = {}
        self._precompute_filters()
        
    def _precompute_filters(self):
        """é è¨ˆç®—æ‰€æœ‰é »ç‡å¸¶çš„SOSæ¿¾æ³¢å™¨"""
        try:
            nyquist = self.sample_rate / 2
            filter_order = 2  # è¼ƒä½éšæ•¸ä»¥ç²å¾—æ›´å¥½æ€§èƒ½
            
            for name, (low, high) in self.bands.items():
                try:
                    # ç¢ºä¿é »ç‡ç¯„åœæœ‰æ•ˆ
                    if high >= nyquist:
                        high = nyquist - 1
                    if low >= high or low <= 0:
                        continue
                        
                    # å»ºç«‹å¸¶é€šæ¿¾æ³¢å™¨
                    sos = butter(
                        filter_order,
                        [low / nyquist, high / nyquist],
                        btype='band',
                        output='sos'
                    )
                    
                    self.sos_filters[name] = sos
                    logger.debug(f"Filter created for {name}: {low}-{high} Hz")
                    
                except Exception as e:
                    logger.warning(f"Failed to create filter for {name}: {e}")
                    continue
                    
            logger.info(f"ğŸ“ å·²å»ºç«‹å„ªåŒ–æ¿¾æ³¢å™¨: {len(self.sos_filters)} å€‹é »ç‡å¸¶")
            
        except Exception as e:
            logger.error(f"é è¨ˆç®—æ¿¾æ³¢å™¨éŒ¯èª¤: {e}")
            
    def _apply_single_filter(self, data: np.ndarray, sos: np.ndarray, 
                           band_name: str) -> np.ndarray:
        """å°è³‡æ–™å¥—ç”¨å–®ä¸€æ¿¾æ³¢å™¨"""
        try:
            if len(data) < 10:  # æœ€å°è³‡æ–™é•·åº¦
                return np.zeros_like(data)
                
            # å¥—ç”¨æ¿¾æ³¢å™¨
            filtered = sosfiltfilt(sos, data)
            return filtered
            
        except Exception as e:
            logger.warning(f"Filter application failed for {band_name}: {e}")
            return np.zeros_like(data)
            
    def process_bands_parallel(self, data: np.ndarray, 
                              use_cache: bool = True) -> Dict[str, np.ndarray]:
        """ä¸¦è¡Œè™•ç†æ‰€æœ‰é »ç‡å¸¶"""
        if len(data) < 10:
            return {name: np.zeros_like(data) for name in self.bands.keys()}
            
        try:
            # ç”¨æ–¼ç·©å­˜çš„è³‡æ–™è®Šæ›´æª¢æ¸¬
            if use_cache:
                data_hash = hash(data.tobytes())
                if (self.last_data_hash == data_hash and 
                    data_hash in self.cache):
                    return self.cache[data_hash]
                    
            # æäº¤æ‰€æœ‰æ¿¾æ³¢ä»»å‹™
            futures = {}
            for name, sos in self.sos_filters.items():
                future = self.executor.submit(
                    self._apply_single_filter, data, sos, name
                )
                futures[name] = future
                
            # åœ¨é€¾æ™‚å…§æ”¶é›†çµæœ
            results = {}
            timeout = 0.05  # æ¯å€‹æ¿¾æ³¢å™¨é€¾æ™‚50æ¯«ç§’
            
            for name, future in futures.items():
                try:
                    result = future.result(timeout=timeout)
                    results[name] = result
                except Exception as e:
                    logger.warning(f"Filter timeout/error for {name}: {e}")
                    results[name] = np.zeros_like(data)
                    
            # ç·©å­˜çµæœ
            if use_cache:
                with self.lock:
                    self.cache[data_hash] = results
                    self.last_data_hash = data_hash
                    
                    # é™åˆ¶ç·©å­˜å¤§å°
                    if len(self.cache) > 10:  # åªä¿ç•™æœ€è¿‘10å€‹
                        oldest_key = next(iter(self.cache))
                        del self.cache[oldest_key]
                        
            return results
            
        except Exception as e:
            logger.error(f"ä¸¦è¡Œè™•ç†éŒ¯èª¤: {e}")
            return {name: np.zeros_like(data) for name in self.bands.keys()}
            
    def compute_band_powers(self, data: np.ndarray) -> Dict[str, float]:
        """è¨ˆç®—æ¯å€‹é »ç‡å¸¶çš„åŠŸç‡"""
        try:
            filtered_data = self.process_bands_parallel(data)
            
            band_powers = {}
            for name, filtered_signal in filtered_data.items():
                if len(filtered_signal) > 0:
                    # è¨ˆç®—RMSåŠŸç‡
                    power = np.mean(filtered_signal ** 2)
                    band_powers[name] = float(power)
                else:
                    band_powers[name] = 0.0
                    
            return band_powers
            
        except Exception as e:
            logger.error(f"è¨ˆç®—é »ç‡å¸¶åŠŸç‡éŒ¯èª¤: {e}")
            return {name: 0.0 for name in self.bands.keys()}
            
    def compute_relative_powers(self, data: np.ndarray) -> Dict[str, float]:
        """è¨ˆç®—æ¯å€‹é »ç‡å¸¶çš„ç›¸å°åŠŸç‡"""
        try:
            band_powers = self.compute_band_powers(data)
            
            # è¨ˆç®—ç¸½åŠŸç‡
            total_power = sum(band_powers.values())
            
            if total_power > 0:
                relative_powers = {
                    name: (power / total_power) * 100 
                    for name, power in band_powers.items()
                }
            else:
                relative_powers = {name: 0.0 for name in band_powers.keys()}
                
            return relative_powers
            
        except Exception as e:
            logger.error(f"è¨ˆç®—ç›¸å°åŠŸç‡éŒ¯èª¤: {e}")
            return {name: 0.0 for name in self.bands.keys()}
            
    def analyze_cognitive_state(self, data: np.ndarray) -> Dict[str, float]:
        """å¾EEGè³‡æ–™åˆ†æèªçŸ¥ç‹€æ…‹"""
        try:
            relative_powers = self.compute_relative_powers(data)
            
            # èªçŸ¥ç‹€æ…‹æŒ‡æ¨™
            cognitive_state = {
                'relaxation': relative_powers.get('alpha', 0),
                'attention': relative_powers.get('beta', 0),
                'drowsiness': relative_powers.get('theta', 0),
                'deep_sleep': relative_powers.get('delta', 0),
                'focus': relative_powers.get('smr', 0),
                'motor_activity': relative_powers.get('mu', 0),
                'high_cognitive': relative_powers.get('gamma', 0)
            }
            
            # è¨ˆç®—ç¶œåˆåˆ†æ•¸
            cognitive_state['alertness'] = (
                cognitive_state['beta'] + cognitive_state['gamma']
            ) / 2
            
            cognitive_state['calmness'] = (
                cognitive_state['alpha'] - cognitive_state['beta']
            )
            
            return cognitive_state
            
        except Exception as e:
            logger.error(f"åˆ†æèªçŸ¥ç‹€æ…‹éŒ¯èª¤: {e}")
            return {
                'relaxation': 0.0, 'attention': 0.0, 'drowsiness': 0.0,
                'deep_sleep': 0.0, 'focus': 0.0, 'motor_activity': 0.0,
                'high_cognitive': 0.0, 'alertness': 0.0, 'calmness': 0.0
            }
            
    def get_performance_stats(self) -> Dict[str, Any]:
        """å–å¾—æ€§èƒ½çµ±è¨ˆè³‡æ–™"""
        with self.lock:
            return {
                'cache_size': len(self.cache),
                'cache_hit_rate': getattr(self, '_cache_hits', 0) / max(getattr(self, '_cache_requests', 1), 1),
                'active_filters': len(self.sos_filters),
                'available_bands': list(self.bands.keys()),
                'executor_active': not self.executor._shutdown
            }
            
    def cleanup(self):
        """æ¸…ç†è³‡æº"""
        try:
            self.executor.shutdown(wait=True)
            with self.lock:
                self.cache.clear()
            logger.info("æ¿¾æ³¢è™•ç†å™¨å·²æ¸…ç†")
            
        except Exception as e:
            logger.error(f"æ¸…ç†æœŸé–“éŒ¯èª¤: {e}")
            
    def __del__(self):
        """è§£æ§‹å‡½æ•¸"""
        self.cleanup()


class AdaptiveFilterProcessor(OptimizedFilterProcessor):
    """å…·æœ‰å‹•æ…‹å„ªåŒ–çš„è‡ªé©æ‡‰æ¿¾æ³¢è™•ç†å™¨"""
    
    def __init__(self, sample_rate: int = 512, max_workers: int = 4):
        super().__init__(sample_rate, max_workers)
        
        # è‡ªé©æ‡‰åƒæ•¸
        self.performance_history = []
        self.adaptive_timeout = 0.05
        self.quality_threshold = 0.8
        
    def _update_performance_metrics(self, processing_time: float, 
                                  data_length: int, success_rate: float):
        """æ›´æ–°è‡ªé©æ‡‰å„ªåŒ–çš„æ€§èƒ½æŒ‡æ¨™"""
        metric = {
            'timestamp': time.time(),
            'processing_time': processing_time,
            'data_length': data_length,
            'success_rate': success_rate,
            'throughput': data_length / processing_time if processing_time > 0 else 0
        }
        
        self.performance_history.append(metric)
        
        # åªä¿ç•™æœ€è¿‘çš„æ­·å²è¨˜éŒ„
        if len(self.performance_history) > 100:
            self.performance_history = self.performance_history[-50:]
            
        # æ ¹æ“šæ€§èƒ½èª¿æ•´é€¾æ™‚æ™‚é–“
        if len(self.performance_history) >= 10:
            avg_time = np.mean([m['processing_time'] for m in self.performance_history[-10:]])
            if avg_time > self.adaptive_timeout:
                self.adaptive_timeout = min(0.1, avg_time * 1.2)
            else:
                self.adaptive_timeout = max(0.02, avg_time * 0.8)
                
    def process_bands_parallel(self, data: np.ndarray, 
                              use_cache: bool = True) -> Dict[str, np.ndarray]:
        """å…·æœ‰æ€§èƒ½ç›£æ§çš„è‡ªé©æ‡‰ä¸¦è¡Œè™•ç†"""
        start_time = time.time()
        
        try:
            # ä½¿ç”¨çˆ¶é¡æ–¹æ³•é™„å¸¶è‡ªé©æ‡‰é€¾æ™‚
            original_timeout = 0.05
            
            # æš«æ™‚è¦†å¯«æœªä¾†ä¸­çš„é€¾æ™‚
            results = super().process_bands_parallel(data, use_cache)
            
            # è¨ˆç®—æˆåŠŸç‡
            success_count = sum(1 for r in results.values() if np.any(r != 0))
            success_rate = success_count / len(results) if results else 0
            
            # æ›´æ–°æ€§èƒ½æŒ‡æ¨™
            processing_time = time.time() - start_time
            self._update_performance_metrics(processing_time, len(data), success_rate)
            
            return results
            
        except Exception as e:
            logger.error(f"è‡ªé©æ‡‰è™•ç†éŒ¯èª¤: {e}")
            return {name: np.zeros_like(data) for name in self.bands.keys()}
            
    def get_adaptive_stats(self) -> Dict[str, Any]:
        """å–å¾—è‡ªé©æ‡‰æ€§èƒ½çµ±è¨ˆè³‡æ–™"""
        base_stats = self.get_performance_stats()
        
        if len(self.performance_history) > 0:
            recent_metrics = self.performance_history[-10:]
            base_stats.update({
                'adaptive_timeout': self.adaptive_timeout,
                'avg_processing_time': np.mean([m['processing_time'] for m in recent_metrics]),
                'avg_throughput': np.mean([m['throughput'] for m in recent_metrics]),
                'avg_success_rate': np.mean([m['success_rate'] for m in recent_metrics]),
                'performance_samples': len(self.performance_history)
            })
        else:
            base_stats.update({
                'adaptive_timeout': self.adaptive_timeout,
                'avg_processing_time': 0,
                'avg_throughput': 0,
                'avg_success_rate': 0,
                'performance_samples': 0
            })
            
        return base_stats